{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kWXtTvGk6fxU"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVm9yUiMVvaq"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "i_-_gLHL6fxZ",
    "outputId": "5a2e4264-0335-4bf3-e06e-beddbf2d5d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>رعد كامل عون</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5827</th>\n",
       "      <td>أنسام شهاب مسلم</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15643</th>\n",
       "      <td>العطاء إقتباسات محبب</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>للاج رمداغ مرح</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>كنزى عواد عبيدة</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>سيرين كلاب ساري</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>آيات باسق نظمي</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>نوار ظفير عبدالحكيم</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>إكليل كاسر عبدالظاهر</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7920</th>\n",
       "      <td>بارع زياد عبدالرشيد</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name  class\n",
       "5734           رعد كامل عون    1.0\n",
       "5827        أنسام شهاب مسلم    1.0\n",
       "15643  العطاء إقتباسات محبب    0.0\n",
       "17998        للاج رمداغ مرح    0.0\n",
       "417         كنزى عواد عبيدة    1.0\n",
       "2153        سيرين كلاب ساري    1.0\n",
       "1725         آيات باسق نظمي    1.0\n",
       "1423    نوار ظفير عبدالحكيم    1.0\n",
       "2465   إكليل كاسر عبدالظاهر    1.0\n",
       "7920    بارع زياد عبدالرشيد    1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\")\n",
    "print(df.shape)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZwHLSn2YlwZ",
    "outputId": "4404909f-d4e1-444f-ea98-bf39fa0bb314"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max number of words per name\n",
    "df['Name'].str.split().str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6j-spTh2joa3"
   },
   "source": [
    "## Document Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "EY57gotK6fxb",
    "outputId": "a64b0f58-8ede-4eaa-97dc-916dfbff41c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SOS> نصر ساعف ياسر <EOS> <SOS> إيمان أخطب قطامي <EOS> <SOS> سعيد عبدالرشيد ضياء <EOS> <SOS> ولهانة صادر جرير <EOS> <SOS> منتظر داني زاكي <EOS> <SOS> منيبة أحنف رافد <EOS> <SOS> مأمون عبودة أزهر <EOS> <SOS> بلقيس برهان مغيث <EOS> <SOS> عماد عبدالرحيم أدهم <EOS> <SOS> هنوف ماهر منير <EOS> <SOS> عبدالعليم صارم نديم <EOS> <SOS> أمل رأفت نصوح <EOS> <SOS> أشهب قسام صلاح <EOS> <SOS> ديمة مأمون راسم <EOS> <SOS> منيع صبيح فلوح <EOS> <SOS> قنوت فواز ساجر <EOS> <SOS> مرحب سليم فوزي <EOS> <SOS> بائقة هواري عزب <EOS> <SOS> خاتم ملحم عفيف <EOS> <SOS> مهرة مسلمة محجن <EOS> <SOS> حصيف خزعل نوري <EOS> <SOS> راجحة غياث سيار <EOS> <SOS> مسفر هيثم غافر <EOS> <SOS> رزان قاهر تيم <EOS> <SOS> بديع بليغ عبدالشكور <EOS> <SOS> أسارير فهد زياد <EOS> <SOS> فوزي ساجد وسام <EOS> <SOS> عفاف تليد صلاح الدين <EOS> <SOS> ساعد فارض سعدون <EOS> <SOS> ميار فتحي راضي <EOS> <SOS> ركين حافظ تغلب <EOS> <SOS> توليب معين نواس <EOS> <SOS> وهبة علاء رسمي <EOS> <SOS> عزة فاضل جواد <EOS> <SOS> ناجي قطامي بهاء <EOS> <SOS> لارا حمد '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding start and end tokens\n",
    "names = ['<SOS> ' + x + ' <EOS>' for x in df['Name'].tolist()]\n",
    "corpus = \" \".join(names)\n",
    "labels = df['class'].tolist()\n",
    "corpus[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o45SDboi2sOu",
    "outputId": "bee099d2-f2a6-4777-b4b1-4e2a44710137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 5096\n",
      "Example of a Sequence\n",
      "[[  2  34 814  86   3   0]\n",
      " [  2 934 287 403   3   0]\n",
      " [  2 177 815 151   3   0]]\n",
      "Some Tokens\n",
      "['<OOV>', '<sos>', '<eos>', 'الدين', 'بكر', 'صلاح', 'معين', 'ناصر', 'نجم', 'نظمي']\n"
     ]
    }
   ],
   "source": [
    "tokenize = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<OOV>\", filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenize.fit_on_texts(names)\n",
    "x = tokenize.texts_to_sequences(names)\n",
    "max_len = 6\n",
    "x = tf.keras.utils.pad_sequences(x, maxlen=max_len, padding='post')\n",
    "total_words = len(tokenize.word_index)+1\n",
    "print(f\"Total Words: {total_words}\")\n",
    "print(\"Example of a Sequence\")\n",
    "print(x[:3])\n",
    "print(\"Some Tokens\")\n",
    "print(list(tokenize.word_index)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data and making dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FfeMdPs5npBO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Validation sizes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16000, 4000, 16000, 4000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x, df['class'].values, test_size=.2, random_state=23)\n",
    "print(\"Train and Validation sizes\")\n",
    "len(x_train), len(x_valid), len(y_train), len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "98tMwyTAhYA2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 09:33:32.993449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 09:33:33.118736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 09:33:33.119025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 09:33:33.120002: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 09:33:33.120708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 09:33:33.120982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 09:33:33.121215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 09:33:33.687296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 09:33:33.687593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 09:33:33.687742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 09:33:33.687847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5325 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Q9KP5pctrdk",
    "outputId": "636fb047-247b-4515-acb3-60f8196b5b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   2 2874  518  615    3    0]\n",
      " [   2 4163 2980 2468    3    0]], shape=(2, 6), dtype=int32) tf.Tensor([1. 0.], shape=(2,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for x,y in ds_train.take(1):\n",
    "  print(x[:2], y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqU-4LG9kOpz"
   },
   "source": [
    "# Build and Train a Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3xPUH48medT_"
   },
   "outputs": [],
   "source": [
    "vocab_size = total_words\n",
    "embedding_dim = 128\n",
    "sequence_length = max_len\n",
    "rnn_units = 512\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZ6sz4au6fxd",
    "outputId": "9b2a4295-0100-486e-b649-f7b3d03b140e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         652288    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 652,417\n",
      "Trainable params: 652,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(vocab_size, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1)],\n",
    "  )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pwJLHKCBPhXz"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "687E5D8HkYpO",
    "outputId": "698d7679-867e-43cd-9c47-2ccca77f6040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 2s 4ms/step - loss: 0.6319 - binary_accuracy: 0.5026 - val_loss: 0.5203 - val_binary_accuracy: 0.5530\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.3637 - binary_accuracy: 0.9116 - val_loss: 0.2415 - val_binary_accuracy: 0.9855\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.1589 - binary_accuracy: 0.9947 - val_loss: 0.1202 - val_binary_accuracy: 0.9937\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0838 - binary_accuracy: 0.9969 - val_loss: 0.0749 - val_binary_accuracy: 0.9940\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0536 - binary_accuracy: 0.9977 - val_loss: 0.0539 - val_binary_accuracy: 0.9945\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0383 - binary_accuracy: 0.9978 - val_loss: 0.0425 - val_binary_accuracy: 0.9948\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0292 - binary_accuracy: 0.9981 - val_loss: 0.0356 - val_binary_accuracy: 0.9948\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0233 - binary_accuracy: 0.9985 - val_loss: 0.0309 - val_binary_accuracy: 0.9945\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.0190 - binary_accuracy: 0.9989 - val_loss: 0.0278 - val_binary_accuracy: 0.9948\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.0162 - binary_accuracy: 0.9988 - val_loss: 0.0255 - val_binary_accuracy: 0.9945\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yn2kSoBd6ZzZ",
    "outputId": "9f6aa72e-fc5a-4680-abbe-f2a9404bd394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9642587]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = '<sos> ' + \"معاذ طه عوض\" + ' <eos>'\n",
    "example_vect = tokenize.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(model.predict(example_vect)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLR-BIdk8XTA",
    "outputId": "9c50845a-ddf3-492b-f9c4-c44085c365b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.87165827]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = \"باسم وحيد السيد\"\n",
    "example = '<sos> ' + example + ' <eos>'\n",
    "example_vect = tokenize.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(model.predict(example_vect)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VzOUDGfQ8d8U",
    "outputId": "bf1f673c-3c39-4419-e4fa-98041c0f841f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.12584206]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = \"شسي شسي شسي\"\n",
    "example = '<sos> ' + example + ' <eos>'\n",
    "example_vect = tokenize.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(model.predict(example_vect)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DHcdPDQ8nxX",
    "outputId": "d680d594-337d-4c66-a409-0672a7425a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.12584206]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = \"باسمم وحةد السد\"\n",
    "example = '<sos> ' + example + ' <eos>'\n",
    "example_vect = tokenize.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(model.predict(example_vect)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfs4ZdQM8wuH",
    "outputId": "93f13a3e-d519-4838-cc22-71c7f42b9790"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99351066]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = \"مريم محمد محمد\"\n",
    "example = '<sos> ' + example + ' <eos>'\n",
    "example_vect = tokenize.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(model.predict(example_vect)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Un6Cc9p8011",
    "outputId": "6c4cb565-e377-419d-faae-17f78b232ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99351066]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = \"محمد مريم محمد\"\n",
    "example = '<sos> ' + example + ' <eos>'\n",
    "example_vect = tokenize.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(model.predict(example_vect)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YuKyjXz88l2",
    "outputId": "13b52215-8659-4826-ebbf-2be6834dd689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6659465]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = \"مريم محمد الرمال\"\n",
    "example = '<sos> ' + example + ' <eos>'\n",
    "example_vect = tokenize.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(model.predict(example_vect)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fb71_3lj9Bbt",
    "outputId": "0536b9e8-0784-46bf-c9f3-7154297554d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.96503353]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = \"محمد طه عوض\"\n",
    "example = '<sos> ' + example + ' <eos>'\n",
    "example_vect = tokenize.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(model.predict(example_vect)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iImTdytn9OR7",
    "outputId": "820c8b25-c786-4466-92f2-d2e5fcecfa37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.75257754]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = \"زيادد عبدالرحمنت محمد\"\n",
    "example = '<sos> ' + example + ' <eos>'\n",
    "example_vect = tokenize.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(model.predict(example_vect)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7krT2wQyUvN",
    "outputId": "39f9a18f-a349-4d8e-aaef-d5057de78ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9415555]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = \"محمد طه عوض لاشين\"\n",
    "example = '<sos> ' + example + ' <eos>'\n",
    "example_vect = tokenize.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(model.predict(example_vect)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/base_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('models/base_model', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         652288    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 652,417\n",
      "Trainable params: 652,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('models/base_model')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('models/tokenizer.pickle', 'rb') as handle:\n",
    "    new_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9415555]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new\n",
    "example = \"محمد طه عوض لاشين\"\n",
    "example = '<sos> ' + example + ' <eos>'\n",
    "example_vect = new_tokenizer.texts_to_sequences([example])\n",
    "tf.nn.sigmoid(new_model.predict(example_vect)).numpy()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
